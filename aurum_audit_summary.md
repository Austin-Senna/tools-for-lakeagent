# Aurum Legacy â†’ New Code Audit & Architecture Summary

> Generated 2026-02-12. Covers the legacy codebase at `aurum_legacy/` and the
> Opus-generated rewrite at `aurum/`.

---

## 1  Feature-Reference Importance Verification

Every ğŸ”´ IMPORTANT rating in `aurum_feature_reference.md` was checked against
the **actual** legacy source code.  
**Result:** all ratings confirmed TRUE.

| Area | Finding |
|------|---------|
| Algorithms & thresholds | Accurate throughout (MinHash 0.7/512, numeric overlap 0.85, inclusion 0.3, PKFK cardinality > 0.7, DBSCAN eps=0.1, join_overlap 0.4, max_hops 5) |
| Function signatures | Frequently inaccurate in the reference doc â€” param names simplified, `store` params added where they don't exist |
| `init_meta_schema` | Takes a **fields iterable**, not a store object |
| `find_path_table` | Requires an **api** param (not mentioned in reference) |
| `virtual_schema_iterative_search` | Doesn't have 5 cleanly named stages â€” only 2 are timed |
| `read_relation(path)` | Has **1** param, not 2 |
| `apply_filter` | First param is a **path string**, not a DataFrame |
| `materialize_join_graph` | Second param is **dod** instance, not filters |

---

## 2  New-Code Audit (aurum/ vs aurum_legacy/)

### 2.1  Verdict Table

| # | Feature | Verdict | Severity |
|---|---------|---------|----------|
| 1 | `compute_field_id` (CRC32) | ğŸŸ¡ **DUAL ID CONFLICT** â€” `ColumnProfile` uses MD5, `Hit` uses CRC32 â†’ IDs never match | ğŸ”´ Critical |
| 2 | `Hit.__hash__` on `int(nid)` | ğŸŸ¡ **CRASH** â€” `int()` on MD5 hex without `base=16` | ğŸ”´ Critical |
| 3 | `Relation` enum values | âœ… Correct | â€” |
| 4 | DRS set ops + provenance | âœ… Correct | â€” |
| 5 | `init_from_profiles` (graph skeleton) | âœ… Correct | â€” |
| 6 | `add_relation` typed edges | âœ… Correct | â€” |
| 7 | `neighbors_id` return type | ğŸŸ¡ Returns bare list instead of DRS (provenance at Algebra layer) | âš ï¸ Minor |
| 8 | `find_path` provenance assembly | ğŸ”´ **NO hop-by-hop provenance chain** â€” returns bare list | ğŸ”´ Critical |
| 9 | `find_path_table` (table-level DFS) | ğŸ”´ **MISSING** entirely | ğŸ”´ Critical |
| 10 | Schema-sim (TF-IDF â†’ LSH) | ğŸŸ¡ O(nÂ²) cosine instead of LSH; `schema_sim_threshold` commented out â†’ crash | ğŸ”´ Crash |
| 11 | MinHash LSH (0.7, 512) | âœ… Correct | â€” |
| 12 | Numeric overlap (0.85 / 0.3 / DBSCAN) | âœ… Correct | â€” |
| 13 | PKFK (cardinality > 0.7) | âœ… Correct | â€” |
| 14 | DoD `virtual_schema_iterative_search` | ğŸŸ¡ Simplified greedy â€” no validation stage, no backup groups | âš ï¸ Major |
| 15 | `joinable()` enumeration | ğŸŸ¡ Simplified â€” no dedup, no unjoinable cache | âš ï¸ Major |
| 16 | `is_join_graph_materializable` | ğŸ”´ **MISSING** â€” no trial-join validation | ğŸ”´ Critical |
| 17 | `join_ab_on_key_optimizer` timeout | ğŸŸ¡ Polars chunks exist but 3-min timeout **never enforced**, no disk spill | âš ï¸ Major |
| 18 | `materialize_join_graph` tree-fold | ğŸŸ¡ Sequential left-to-right instead of tree-fold â€” breaks non-linear graphs | âš ï¸ Major |
| 19 | Config values | âœ… Values correct but `schema_sim_threshold` commented out; `aurumConfig` missing | ğŸ”´ Crash |

### 2.2  Critical Bugs

1. **Dual ID system** â€” `ColumnProfile.nid` is MD5 hex, `compute_field_id` is CRC32 â†’ nodes indexed by one, looked up by the other â†’ guaranteed mismatch.
2. **`Hit.__hash__`** calls `int(nid)` on MD5 hex â†’ `ValueError` at runtime.
3. **`make_drs`** calls `compute_field_id` with 2 args, but it requires 3 â†’ `TypeError`.
4. **`schema_sim_threshold`** is commented out â†’ `AttributeError` when `build_schema_sim` runs.

---

## 3  Legacy Architecture Schema

### 3.1  Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data Sources     â”‚  CSV files, databases, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ddprofiler      â”‚  Java â€” profiles every column
    â”‚  â†’ Elasticsearch â”‚  index: 'profile'
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  per-column: nid, dbName, sourceName,
           â”‚             columnName, dataType, minhash sigs,
           â”‚             num sigs, totalValues, uniqueValues, path
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  networkbuildercoordinator.main(output_path)        â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
    â”‚  1. store.get_all_fields()                          â”‚
    â”‚     â†’ generator of (nid, db, src, field, total,     â”‚
    â”‚       unique, dataType)                             â”‚
    â”‚                                                      â”‚
    â”‚  2. network.init_meta_schema(fields)                â”‚
    â”‚     [fieldnetwork.py]                               â”‚
    â”‚     â†’ graph nodes + id_names + source_ids           â”‚
    â”‚                                                      â”‚
    â”‚  3. build_schema_sim_relation(network, store)       â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ TF-IDF on column names â†’ NearPy LSH           â”‚
    â”‚     â†’ SCHEMA_SIM edges                               â”‚
    â”‚                                                      â”‚
    â”‚  4. build_content_sim_mh_text(network, mh_sigs)     â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ MinHashLSH(threshold=0.7, num_perm=512)       â”‚
    â”‚     â†’ CONTENT_SIM edges (text columns)               â”‚
    â”‚                                                      â”‚
    â”‚  5. build_content_sim_num_overlap_distr(net, sigs)   â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ medianÂ±IQR overlap â‰¥ 0.85 â†’ CONTENT_SIM       â”‚
    â”‚     â†’ core overlap â‰¥ 0.3 â†’ INCLUSION_DEPENDENCY      â”‚
    â”‚     â†’ DBSCAN(eps=0.1) for single-point columns       â”‚
    â”‚                                                      â”‚
    â”‚  6. build_pkfk_relation(network, store)             â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ cardinality ratio > 0.7 â†’ PKFK edges          â”‚
    â”‚                                                      â”‚
    â”‚  7. serialize_network(path)                         â”‚
    â”‚     â†’ graph.pickle, id_info.pickle,                  â”‚
    â”‚       table_ids.pickle, lsh_indexes                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Serialized Model (pickle files on disk)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  main.init_system(path)  [main.py]                  â”‚
    â”‚  â†’ network = deserialize_network(path)              â”‚
    â”‚  â†’ store   = StoreHandler(config)                   â”‚
    â”‚  â†’ api     = API(network, store)   [ddapi.py]       â”‚
    â”‚    (API inherits Algebra)                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  User Queries via Algebra  [algebra.py]             â”‚
    â”‚                                                      â”‚
    â”‚  api.search_content("salary")     â†’ DRS             â”‚
    â”‚  api.content_similar_to(drs)      â†’ DRS             â”‚
    â”‚  api.pkfk_of(drs)                â†’ DRS             â”‚
    â”‚  api.paths(drs_a, drs_b, PKFK)   â†’ DRS             â”‚
    â”‚  api.intersection(a, b)           â†’ DRS             â”‚
    â”‚  drs.why(hit)  /  drs.how(hit)   â†’ provenance      â”‚
    â”‚  drs.rank_certainty()  /  rank_coverage()           â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Data on Demand  [DoD/dod.py]                       â”‚
    â”‚                                                      â”‚
    â”‚  Input: list_attributes + list_values               â”‚
    â”‚                                                      â”‚
    â”‚  Stage 1 â€” Search filters                           â”‚
    â”‚    api.search_exact_attribute(attr)  â†’ DRS          â”‚
    â”‚    api.search_content(value)         â†’ DRS          â”‚
    â”‚    intersect where both specified                    â”‚
    â”‚                                                      â”‚
    â”‚  Stage 2 â€” Candidate group formation                â”‚
    â”‚    group tables by filter coverage                   â”‚
    â”‚    greedy enumeration w/ pivot exploration            â”‚
    â”‚                                                      â”‚
    â”‚  Stage 3 â€” Join graph discovery                     â”‚
    â”‚    api.paths(t1, t2, PKFK)  per pair                â”‚
    â”‚    itertools.product â†’ covering join graphs          â”‚
    â”‚                                                      â”‚
    â”‚  Stage 4 â€” Materializability check                  â”‚
    â”‚    is_join_graph_materializable()                    â”‚
    â”‚    trial join per hop; reject if 0 rows              â”‚
    â”‚                                                      â”‚
    â”‚  Stage 5 â€” Materialization                          â”‚
    â”‚    [data_processing_utils.py]                        â”‚
    â”‚    materialize_join_graph (tree-fold)                â”‚
    â”‚    join_ab_on_key_optimizer (3-min timeout)          â”‚
    â”‚    project requested columns                         â”‚
    â”‚    yield (materialized_view, metadata)               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2  Key Data Structures

#### FieldNetwork  (`knowledgerepr/fieldnetwork.py`)

| Component | Type | Description |
|-----------|------|-------------|
| `graph` | `nx.MultiGraph` | Nodes = column nids (CRC32). Node attr `cardinality` = unique/total. Edges keyed by `Relation`, carry `score`. |
| `id_names` | `dict[str, tuple]` | nid â†’ `(db_name, source_name, field_name, data_type)` |
| `source_ids` | `defaultdict(list)` | `source_name` â†’ `[nid, ...]` |

#### Hit  (`api/apiutils.py`)

```python
Hit = namedtuple('Hit', 'nid, db_name, source_name, field_name, score')
# Identity: hash on int(nid), equality on nid
# nid = str(binascii.crc32(bytes(db + source + field, 'utf8')))
```

#### DRS â€” Domain Result Set  (`api/apiutils.py`)

| Property | Type | Description |
|----------|------|-------------|
| `data` | `set[Hit]` | The result set |
| `provenance` | `Provenance` | DAG tracking derivation |
| `operation` | `OP` | The op that created this DRS |
| `score` | `dict[Hit, float]` | Per-element ranking scores |

#### Provenance  (`api/apiutils.py`)

- Backed by `nx.MultiDiGraph`
- **Nodes** = `Hit` objects (including synthetic origin Hits for keyword searches)
- **Edges** = labeled with `OP` enum values
- **Leafs** = origin nodes (no predecessors)
- **Heads** = terminal nodes (no successors)

#### Relation Enum  (`api/apiutils.py`)

| Name | Value | Built By |
|------|-------|----------|
| `SCHEMA` | 0 | `init_meta_schema` (same-table) |
| `SCHEMA_SIM` | 1 | `build_schema_sim_relation` (TF-IDF + NearPy) |
| `CONTENT_SIM` | 2 | `build_content_sim_mh_text` / `build_content_sim_num_overlap_distr` |
| `ENTITY_SIM` | 3 | *(disabled)* |
| `PKFK` | 5 | `build_pkfk_relation` |
| `INCLUSION_DEPENDENCY` | 6 | `build_content_sim_num_overlap_distr` |

### 3.3  Entry Points

| Task | File | Command |
|------|------|---------|
| Build network | `networkbuildercoordinator.py` | `python networkbuildercoordinator.py --opath /output/` |
| Query interactively | `main.py` | `python main.py --path_to_model /model/` |
| Run DoD | `run_dod.py` | `python run_dod.py --model_path /model/ --list_attributes "A;B" --list_values "v1;v2"` |

### 3.4  External Dependencies

| Dependency | Role |
|------------|------|
| **Elasticsearch** | Persistent column-profile store + keyword search |
| **NetworkX** | MultiGraph (field network), MultiDiGraph (provenance) |
| **scikit-learn** | `TfidfVectorizer` for schema similarity |
| **NearPy** | LSH (RandomBinaryProjections) for schema/content sim |
| **datasketch** | MinHash + MinHashLSH for text content similarity |
| **pandas** | DataFrame operations for join materialization |
| **psutil** | Memory-limit estimation during joins |
| **NumPy / SciPy** | Numerical analysis, distribution comparison |
