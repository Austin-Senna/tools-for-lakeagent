# Aurum Legacy â†’ New Code Audit & Architecture Summary

> Generated 2026-02-12. Updated 2026-02-13.  
> Covers the legacy codebase at `aurum_legacy/`, the Opus-generated rewrite at
> `aurum/`, and the clean skeleton at `aurum_v2/`.

---

## 1  Feature-Reference Importance Verification

Every ğŸ”´ IMPORTANT rating in `aurum_feature_reference.md` was checked against
the **actual** legacy source code.  
**Result:** all ratings confirmed TRUE.

| Area | Finding |
|------|---------|
| Algorithms & thresholds | Accurate throughout (MinHash 0.7/512, numeric overlap 0.85, inclusion 0.3, PKFK cardinality > 0.7, DBSCAN eps=0.1, join_overlap 0.4, max_hops 5) |
| Function signatures | Frequently inaccurate in the reference doc â€” param names simplified, `store` params added where they don't exist |
| `init_meta_schema` | Takes a **fields iterable**, not a store object |
| `find_path_table` | Requires an **api** param (not mentioned in reference) |
| `virtual_schema_iterative_search` | Doesn't have 5 cleanly named stages â€” only 2 are timed |
| `read_relation(path)` | Has **1** param, not 2 |
| `apply_filter` | First param is a **path string**, not a DataFrame |
| `materialize_join_graph` | Second param is **dod** instance, not filters |

---

## 2  New-Code Audit (aurum/ vs aurum_legacy/)

### 2.1  Verdict Table

| # | Feature | Verdict | Severity |
|---|---------|---------|----------|
| 1 | `compute_field_id` (CRC32) | ğŸŸ¡ **DUAL ID CONFLICT** â€” `ColumnProfile` uses MD5, `Hit` uses CRC32 â†’ IDs never match | ğŸ”´ Critical |
| 2 | `Hit.__hash__` on `int(nid)` | ğŸŸ¡ **CRASH** â€” `int()` on MD5 hex without `base=16` | ğŸ”´ Critical |
| 3 | `Relation` enum values | âœ… Correct | â€” |
| 4 | DRS set ops + provenance | âœ… Correct | â€” |
| 5 | `init_from_profiles` (graph skeleton) | âœ… Correct | â€” |
| 6 | `add_relation` typed edges | âœ… Correct | â€” |
| 7 | `neighbors_id` return type | ğŸŸ¡ Returns bare list instead of DRS (provenance at Algebra layer) | âš ï¸ Minor |
| 8 | `find_path` provenance assembly | ğŸ”´ **NO hop-by-hop provenance chain** â€” returns bare list | ğŸ”´ Critical |
| 9 | `find_path_table` (table-level DFS) | ğŸ”´ **MISSING** entirely | ğŸ”´ Critical |
| 10 | Schema-sim (TF-IDF â†’ LSH) | ğŸŸ¡ O(nÂ²) cosine instead of LSH; `schema_sim_threshold` commented out â†’ crash | ğŸ”´ Crash |
| 11 | MinHash LSH (0.7, 512) | âœ… Correct | â€” |
| 12 | Numeric overlap (0.85 / 0.3 / DBSCAN) | âœ… Correct | â€” |
| 13 | PKFK (cardinality > 0.7) | âœ… Correct | â€” |
| 14 | DoD `virtual_schema_iterative_search` | ğŸŸ¡ Simplified greedy â€” no validation stage, no backup groups | âš ï¸ Major |
| 15 | `joinable()` enumeration | ğŸŸ¡ Simplified â€” no dedup, no unjoinable cache | âš ï¸ Major |
| 16 | `is_join_graph_materializable` | ğŸ”´ **MISSING** â€” no trial-join validation | ğŸ”´ Critical |
| 17 | `join_ab_on_key_optimizer` timeout | ğŸŸ¡ Polars chunks exist but 3-min timeout **never enforced**, no disk spill | âš ï¸ Major |
| 18 | `materialize_join_graph` tree-fold | ğŸŸ¡ Sequential left-to-right instead of tree-fold â€” breaks non-linear graphs | âš ï¸ Major |
| 19 | Config values | âœ… Values correct but `schema_sim_threshold` commented out; `aurumConfig` missing | ğŸ”´ Crash |

### 2.2  Critical Bugs

1. **Dual ID system** â€” `ColumnProfile.nid` is MD5 hex, `compute_field_id` is CRC32 â†’ nodes indexed by one, looked up by the other â†’ guaranteed mismatch.
2. **`Hit.__hash__`** calls `int(nid)` on MD5 hex â†’ `ValueError` at runtime.
3. **`make_drs`** calls `compute_field_id` with 2 args, but it requires 3 â†’ `TypeError`.
4. **`schema_sim_threshold`** is commented out â†’ `AttributeError` when `build_schema_sim` runs.

---

## 3  Legacy Architecture Schema

### 3.1  Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data Sources     â”‚  CSV files, databases, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ddprofiler      â”‚  Java â€” profiles every column
    â”‚  â†’ Elasticsearch â”‚  index: 'profile'
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  per-column: nid, dbName, sourceName,
           â”‚             columnName, dataType, minhash sigs,
           â”‚             num sigs, totalValues, uniqueValues, path
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  networkbuildercoordinator.main(output_path)        â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
    â”‚  1. store.get_all_fields()                          â”‚
    â”‚     â†’ generator of (nid, db, src, field, total,     â”‚
    â”‚       unique, dataType)                             â”‚
    â”‚                                                      â”‚
    â”‚  2. network.init_meta_schema(fields)                â”‚
    â”‚     [fieldnetwork.py]                               â”‚
    â”‚     â†’ graph nodes + id_names + source_ids           â”‚
    â”‚                                                      â”‚
    â”‚  3. build_schema_sim_relation(network, store)       â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ TF-IDF on column names â†’ NearPy LSH           â”‚
    â”‚     â†’ SCHEMA_SIM edges                               â”‚
    â”‚                                                      â”‚
    â”‚  4. build_content_sim_mh_text(network, mh_sigs)     â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ MinHashLSH(threshold=0.7, num_perm=512)       â”‚
    â”‚     â†’ CONTENT_SIM edges (text columns)               â”‚
    â”‚                                                      â”‚
    â”‚  5. build_content_sim_num_overlap_distr(net, sigs)   â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ medianÂ±IQR overlap â‰¥ 0.85 â†’ CONTENT_SIM       â”‚
    â”‚     â†’ core overlap â‰¥ 0.3 â†’ INCLUSION_DEPENDENCY      â”‚
    â”‚     â†’ DBSCAN(eps=0.1) for single-point columns       â”‚
    â”‚                                                      â”‚
    â”‚  6. build_pkfk_relation(network, store)             â”‚
    â”‚     [dataanalysis.py]                               â”‚
    â”‚     â†’ cardinality ratio > 0.7 â†’ PKFK edges          â”‚
    â”‚                                                      â”‚
    â”‚  7. serialize_network(path)                         â”‚
    â”‚     â†’ graph.pickle, id_info.pickle,                  â”‚
    â”‚       table_ids.pickle, lsh_indexes                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Serialized Model (pickle files on disk)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  main.init_system(path)  [main.py]                  â”‚
    â”‚  â†’ network = deserialize_network(path)              â”‚
    â”‚  â†’ store   = StoreHandler(config)                   â”‚
    â”‚  â†’ api     = API(network, store)   [ddapi.py]       â”‚
    â”‚    (API inherits Algebra)                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  User Queries via Algebra  [algebra.py]             â”‚
    â”‚                                                      â”‚
    â”‚  api.search_content("salary")     â†’ DRS             â”‚
    â”‚  api.content_similar_to(drs)      â†’ DRS             â”‚
    â”‚  api.pkfk_of(drs)                â†’ DRS             â”‚
    â”‚  api.paths(drs_a, drs_b, PKFK)   â†’ DRS             â”‚
    â”‚  api.intersection(a, b)           â†’ DRS             â”‚
    â”‚  drs.why(hit)  /  drs.how(hit)   â†’ provenance      â”‚
    â”‚  drs.rank_certainty()  /  rank_coverage()           â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Data on Demand  [DoD/dod.py]                       â”‚
    â”‚                                                      â”‚
    â”‚  Input: list_attributes + list_values               â”‚
    â”‚                                                      â”‚
    â”‚  Stage 1 â€” Search filters                           â”‚
    â”‚    api.search_exact_attribute(attr)  â†’ DRS          â”‚
    â”‚    api.search_content(value)         â†’ DRS          â”‚
    â”‚    intersect where both specified                    â”‚
    â”‚                                                      â”‚
    â”‚  Stage 2 â€” Candidate group formation                â”‚
    â”‚    group tables by filter coverage                   â”‚
    â”‚    greedy enumeration w/ pivot exploration            â”‚
    â”‚                                                      â”‚
    â”‚  Stage 3 â€” Join graph discovery                     â”‚
    â”‚    api.paths(t1, t2, PKFK)  per pair                â”‚
    â”‚    itertools.product â†’ covering join graphs          â”‚
    â”‚                                                      â”‚
    â”‚  Stage 4 â€” Materializability check                  â”‚
    â”‚    is_join_graph_materializable()                    â”‚
    â”‚    trial join per hop; reject if 0 rows              â”‚
    â”‚                                                      â”‚
    â”‚  Stage 5 â€” Materialization                          â”‚
    â”‚    [data_processing_utils.py]                        â”‚
    â”‚    materialize_join_graph (tree-fold)                â”‚
    â”‚    join_ab_on_key_optimizer (3-min timeout)          â”‚
    â”‚    project requested columns                         â”‚
    â”‚    yield (materialized_view, metadata)               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2  Key Data Structures

#### FieldNetwork  (`knowledgerepr/fieldnetwork.py`)

| Component | Type | Description |
|-----------|------|-------------|
| `graph` | `nx.MultiGraph` | Nodes = column nids (CRC32). Node attr `cardinality` = unique/total. Edges keyed by `Relation`, carry `score`. |
| `id_names` | `dict[str, tuple]` | nid â†’ `(db_name, source_name, field_name, data_type)` |
| `source_ids` | `defaultdict(list)` | `source_name` â†’ `[nid, ...]` |

#### Hit  (`api/apiutils.py`)

```python
Hit = namedtuple('Hit', 'nid, db_name, source_name, field_name, score')
# Identity: hash on int(nid), equality on nid
# nid = str(binascii.crc32(bytes(db + source + field, 'utf8')))
```

#### DRS â€” Domain Result Set  (`api/apiutils.py`)

| Property | Type | Description |
|----------|------|-------------|
| `data` | `set[Hit]` | The result set |
| `provenance` | `Provenance` | DAG tracking derivation |
| `operation` | `OP` | The op that created this DRS |
| `score` | `dict[Hit, float]` | Per-element ranking scores |

#### Provenance  (`api/apiutils.py`)

- Backed by `nx.MultiDiGraph`
- **Nodes** = `Hit` objects (including synthetic origin Hits for keyword searches)
- **Edges** = labeled with `OP` enum values
- **Leafs** = origin nodes (no predecessors)
- **Heads** = terminal nodes (no successors)

#### Relation Enum  (`api/apiutils.py`)

| Name | Value | Built By |
|------|-------|----------|
| `SCHEMA` | 0 | `init_meta_schema` (same-table) |
| `SCHEMA_SIM` | 1 | `build_schema_sim_relation` (TF-IDF + NearPy) |
| `CONTENT_SIM` | 2 | `build_content_sim_mh_text` / `build_content_sim_num_overlap_distr` |
| `ENTITY_SIM` | 3 | *(disabled)* |
| `PKFK` | 5 | `build_pkfk_relation` |
| `INCLUSION_DEPENDENCY` | 6 | `build_content_sim_num_overlap_distr` |

### 3.3  Entry Points

| Task | File | Command |
|------|------|---------|
| Build network | `networkbuildercoordinator.py` | `python networkbuildercoordinator.py --opath /output/` |
| Query interactively | `main.py` | `python main.py --path_to_model /model/` |
| Run DoD | `run_dod.py` | `python run_dod.py --model_path /model/ --list_attributes "A;B" --list_values "v1;v2"` |

### 3.4  External Dependencies

| Dependency | Role |
|------------|------|
| **Elasticsearch** | Persistent column-profile store + keyword search |
| **NetworkX** | MultiGraph (field network), MultiDiGraph (provenance) |
| **scikit-learn** | `TfidfVectorizer` for schema similarity |
| **NearPy** | LSH (RandomBinaryProjections) for schema/content sim |
| **datasketch** | MinHash + MinHashLSH for text content similarity |
| **pandas** | DataFrame operations for join materialization |
| **psutil** | Memory-limit estimation during joins |
| **NumPy / SciPy** | Numerical analysis, distribution comparison |

---

## 4  ddprofiler Analysis (Java â†’ Python Replacement Needed)

### 4.1  What It Is

The `ddprofiler` is a **Java 8 application** located at `aurum_legacy/ddprofiler/`.
It is the **first stage of the entire Aurum pipeline** â€” without it, Elasticsearch
has no data, and none of the Python code can function.

### 4.2  Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main.java  â†’  startProfiler(ProfilerConfig)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  1. Creates Store (NativeElasticStore)                 â”‚
â”‚  2. Creates Conductor (thread pool + N Workers)        â”‚
â”‚  3. Parses YAML config â†’ Source objects                â”‚
â”‚     (CSVSource, PostgresSource, HiveSource, etc.)      â”‚
â”‚  4. Submits each Source to Conductor queue              â”‚
â”‚  5. Waits for completion â†’ teardown                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker.java  (per-table/file processing)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  For each column:                                      â”‚
â”‚   1. PreAnalyzer.readRows(chunk) â†’ detect type         â”‚
â”‚      (FLOAT, INT, STRING)                              â”‚
â”‚   2. AnalyzerFactory â†’ TextualAnalysis or              â”‚
â”‚      NumericalAnalysis                                 â”‚
â”‚   3. Feed data chunks iteratively to analyzers         â”‚
â”‚   4. FilterAndBatchDataIndexer â†’ index raw text to ES  â”‚
â”‚      (ES index: "text", type: "column")                â”‚
â”‚   5. Wrap results â†’ WorkerTaskResult                   â”‚
â”‚   6. store.storeDocument(wtr) â†’ profile to ES          â”‚
â”‚      (ES index: "profile", type: "column")             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3  What Gets Stored in Elasticsearch

#### ES Index: `profile` (one document per column)

| Field | Type | Source | Used By |
|-------|------|--------|---------|
| `id` | long | CRC32 of db+source+column | Node ID throughout system |
| `dbName` | keyword | Source config | Hit.db_name |
| `path` | keyword | Source config | File path |
| `sourceName` | text (aurum_analyzer) | Table/file name | Hit.source_name, `search_keywords` |
| `sourceNameNA` | keyword | Table/file name (not analyzed) | Exact match lookups |
| `columnName` | text (aurum_analyzer) | Column name | `search_keywords` |
| `columnNameNA` | keyword | Column name (not analyzed) | Exact match lookups |
| `dataType` | keyword | "T" (text) or "N" (numeric) | Type filtering |
| `totalValues` | long | Row count | Cardinality ratio |
| `uniqueValues` | long | HyperLogLog estimate | Cardinality ratio, PKFK |
| `entities` | keyword | OpenNLP NER labels | Entity similarity |
| `minhash` | long[] | KMinHash (K=512, Mersenne prime) | `build_content_sim_mh_text` |
| `minValue` | double | Column min | Numeric overlap |
| `maxValue` | double | Column max | Numeric overlap |
| `avgValue` | double | Column average | Numeric overlap |
| `median` | long | Column median | `build_content_sim_num_overlap_distr` |
| `iqr` | long | Interquartile range | `build_content_sim_num_overlap_distr` |

#### ES Index: `text` (one document per column, raw values)

| Field | Type | Source | Used By |
|-------|------|--------|---------|
| `id` | long | Column ID | Join back to profile |
| `dbName` | keyword | Source config | â€” |
| `path` | keyword | Source config | â€” |
| `sourceName` | keyword | Table/file name | â€” |
| `columnName` | keyword | Column name | â€” |
| `columnNameSuggest` | completion | Column name | Auto-suggest |
| `text` | text (english analyzer) | Raw column values | `search_content`, keyword search |

### 4.4  Key Profiling Algorithms

| Algorithm | Java Class | Output | Python Equivalent |
|-----------|-----------|--------|-------------------|
| **KMinHash** | `analysis.modules.KMinHash` | `long[512]` â€” 512 min-hash signatures using Mersenne prime `(2^61-1)` | `datasketch.MinHash` |
| **Cardinality** | `analysis.modules.CardinalityAnalyzer` | `uniqueValues` via HyperLogLog (stream lib) | `hyperloglog` or `datasketch.HyperLogLog` |
| **Range** | `analysis.modules.RangeAnalyzer` | min, max, avg, median, IQR | `numpy`/`pandas` describe |
| **NER Entities** | `analysis.modules.EntityAnalyzer` | Entity type labels via OpenNLP | `spacy` or `transformers` NER |
| **Type Detection** | `preanalysis.PreAnalyzer` | FLOAT / INT / STRING per column | `pandas.api.types` |

### 4.5  Recommendation

**YES, aurum_v2 needs a Python-based profiler module.** The ddprofiler is the
critical first stage â€” it creates the ES indices that everything else depends on.
A Python replacement using `pandas` + `datasketch` + `spacy` (optional NER) can
replicate all essential functionality. Proposed module: `aurum_v2/profiler/`.

---

## 5  EKG / Neo4j Analysis

### 5.1  What It Is

The EKG (Enterprise Knowledge Graph) subsystem consists of:
- `EKGapi.py` â€” Abstract graph backend with pluggable backends (IN_MEMORY, POSTGRES, JANUS, NEO, VIRTUOSO, G_INDEX)
- `inmemoryekg.py` â€” `InMemoryEKG(EKGapi)` â€” stub, no methods implemented
- `gindexekg.py` â€” `GIndexEKG(EKGapi)` â€” uses C shared library (`graph_index.so`) + PostgreSQL store
- `export_network_2_neo4j.py` â€” One-way export script: serialized FieldNetwork â†’ Neo4j
- `ekgstore/neo4j_store.py` â€” Neo4j driver for the export

### 5.2  Is It Used by the Core Pipeline?

**NO.** Confirmed by grep:
- `algebra.py` â€” zero references to EKG/Neo4j
- `ddapi.py` â€” zero references to EKG/Neo4j
- `main.py` â€” zero references to EKG/Neo4j
- `networkbuildercoordinator.py` â€” zero references to EKG/Neo4j
- `DoD/dod.py` â€” zero references to EKG/Neo4j

The EKG is a **separate experimental subsystem** â€” an alternative graph backend
that was never integrated into the main query pipeline. The core pipeline uses
`FieldNetwork` (NetworkX in-memory graph) exclusively.

The Neo4j export is a **one-way visualization tool** â€” it exports the pickle-based
network to Neo4j for browsing, but the query engine never reads from Neo4j.

### 5.3  Recommendation

**Not needed in aurum_v2.** The EKG is an experimental add-on with incomplete
implementations (InMemoryEKG is empty, GIndexEKG.neighbors_id returns None).
If Neo4j visualization is desired later, it can be added as an optional export
script outside the core library.

---

## 6  aurum_v2 Completeness Audit

### 6.1  Coverage Summary

| Category | Legacy Modules | In aurum_v2? |
|----------|---------------|--------------|
| **Config** | `config.py` | âœ… `config.py` (AurumConfig dataclass) |
| **Data Models** | `apiutils.py` (Hit, DRS, Relation, OP, Provenance) | âœ… `models/` (4 files) |
| **ES Store** | `inputoutput.py` (StoreHandler) | âœ… `store/elastic_store.py` |
| **Field Network** | `fieldnetwork.py` (FieldNetwork) | âœ… `graph/field_network.py` |
| **Network Builder** | `networkbuildercoordinator.py` + `dataanalysis.py` | âš ï¸ `builder/` (2 files) â€” missing `dataanalysis.py` analysis functions |
| **Algebra** | `algebra.py` | âœ… `discovery/algebra.py` |
| **API** | `ddapi.py` (API + Helper) | âš ï¸ `discovery/api.py` â€” missing ~20 convenience methods |
| **DoD** | `DoD/dod.py` | âœ… `dod/dod.py` |
| **Join Materialization** | `DoD/data_processing_utils.py` | âš ï¸ `dod/join_utils.py` â€” missing 7 functions |
| **View Analysis** | `DoD/material_view_analysis.py` | âŒ **MISSING** â€” ViewClass enum + 8 functions |
| **Text Utils** | `dataanalysis/nlp_utils.py` | âš ï¸ `utils/text_utils.py` â€” missing 4 NLP functions |
| **IO Utils** | `inputoutput/inputoutput.py` (pickle) | âœ… `utils/io_utils.py` |
| **Profiler** | `ddprofiler/` (Java) | âŒ **MISSING** â€” no Python profiler |
| **Annotation/Metadata** | `api/annotation.py` (MRS, MDHit, MDComment) | âŒ **MISSING** â€” entire metadata type system |
| **Reporting** | `api/reporting.py` (Report class) | âŒ **MISSING** â€” graph statistics |
| **Sugar** | `sugar.py` (interactive shortcuts) | âŒ Not needed â€” UX layer |
| **Data Analysis** | `dataanalysis/dataanalysis.py` (25+ functions) | âŒ **MISSING** â€” TF-IDF, KS test, cosine similarity, etc. |
| **EKG/Neo4j** | `knowledgerepr/EKGapi.py` + neo4j export | âŒ Not needed â€” experimental, unused |

### 6.2  Critical Missing Modules (blocks functionality)

| # | Module | What It Does | Priority |
|---|--------|-------------|----------|
| 1 | **Profiler** (`profiler/`) | Reads CSV/DB â†’ computes per-column stats â†’ populates ES | ğŸ”´ CRITICAL â€” without this, no data exists |
| 2 | **Data Analysis** (`builder/analysis.py`) | TF-IDF vectorization, cosine similarity, KS test, distribution overlap â€” called by network builder | ğŸ”´ CRITICAL â€” builder stubs call these |
| 3 | **Annotation** (`models/annotation.py`) | MDClass, MDRelation, MDHit, MDComment, MRS â€” metadata type system | ğŸŸ¡ IMPORTANT â€” needed for metadata features |
| 4 | **View Analysis** (`dod/view_analysis.py`) | ViewClass enum, 4C classification (equivalent/contained/complementary/contradictory) | ğŸŸ¡ IMPORTANT â€” DoD output classification |

### 6.3  Missing Functions in Existing Modules

#### `discovery/api.py` â€” Missing ~20 Convenience Methods

```
make_drs(db, source, field)     drs_from_hit(hit)
drs_from_hits(hits)             drs_from_table(source)
drs_expand_to_table(drs)        search_content(kw)
search_attribute(kw)            search_exact_attribute(kw)
search_exact_source(kw)         search_entity(entity)
similar_content_to(drs)         similar_schema_to(drs)
pkfk_of(drs)                    inclusion_dependency_of(drs)
neighbor_of(drs, rel)           paths_between(a, b, rel)
traverse(a, b, rel)             display_drs(drs)
print_drs(drs)                  Helper class (web formatting)
```

#### `store/elastic_store.py` â€” Missing Methods

```
get_all_fields_of_source(source)    search_fuzzy(kw, type)
get_column_entities(nid)            get_text_signatures(nid)
write_annotation(...)               write_comment(...)
search_annotations(...)             read_annotations(nid)
read_comments(nid)                  sample_col_values(nid)
```

#### `dod/join_utils.py` â€” Missing Functions

```
join_dfs_on_key(df_a, df_b, key)     join_ab_on_key_disk(...)
join_ab_on_key_nan_safe(...)         compute_join_selectivity(...)
filter_by_values(df, filter)         project_columns_alt(...)
estimate_cartesian_memory(...)       profile_column_quality(...)
```

#### `builder/network_builder.py` â€” Missing Analysis Functions

These are in `dataanalysis/dataanalysis.py` in legacy and need to be either
inlined or extracted into a separate analysis module:

```
build_schema_sim_relation()  â€” calls: tf_idf_vectorize(), cosine_sim()
build_content_sim_mh_text()  â€” calls: compute_minhash()
build_content_sim_num()      â€” calls: compute_overlap(), ks_test()
build_entity_sim()           â€” calls: entity_overlap()
build_content_sim_lsa()      â€” alternative: SVD-based
build_schema_sim_lsa()       â€” alternative: LSA schema matching
```

### 6.4  Complete Pipeline Data Flow (Updated)

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ STAGE 0: DATA INGESTION  (âŒ MISSING from aurum_v2)         â”‚
 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
 â”‚                                                             â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
 â”‚  â”‚  Data Sources    â”‚  CSV files, PostgreSQL, Hive, etc.    â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
 â”‚           â”‚                                                  â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
 â”‚  â”‚  Profiler  (legacy: ddprofiler Java)                â”‚    â”‚
 â”‚  â”‚  For each source â†’ for each column:                 â”‚    â”‚
 â”‚  â”‚   â€¢ Detect type (FLOAT/INT/STRING)                  â”‚    â”‚
 â”‚  â”‚   â€¢ Compute KMinHash[512] (text columns)            â”‚    â”‚
 â”‚  â”‚   â€¢ Compute HyperLogLog cardinality                 â”‚    â”‚
 â”‚  â”‚   â€¢ Compute Range stats (min/max/avg/median/IQR)    â”‚    â”‚
 â”‚  â”‚   â€¢ Run NER (date/location/money/org/person/time)   â”‚    â”‚
 â”‚  â”‚   â€¢ Index raw text values for keyword search         â”‚    â”‚
 â”‚  â”‚  Store to ES: "profile" index + "text" index        â”‚    â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
 â”‚           â”‚                                                  â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
 â”‚  â”‚  Elasticsearch                                      â”‚    â”‚
 â”‚  â”‚  â”œâ”€â”€ "profile" index: 1 doc/column (stats + sigs)   â”‚    â”‚
 â”‚  â”‚  â””â”€â”€ "text" index: 1 doc/column (raw values)        â”‚    â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ STAGE 1: NETWORK BUILDING  (âœ… aurum_v2/builder/)           â”‚
 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
 â”‚                                                             â”‚
 â”‚  coordinator.build_network(config):                         â”‚
 â”‚   1. store.get_all_fields()                                 â”‚
 â”‚      â†’ generator: (nid, db, src, field, total, unique, dt)  â”‚
 â”‚   2. network.init_meta_schema(fields)                       â”‚
 â”‚      â†’ graph nodes + id_names + source_ids + SCHEMA edges   â”‚
 â”‚   3. build_schema_sim(network, store)                       â”‚
 â”‚      â†’ TF-IDF on column names â†’ NearPy LSH â†’ SCHEMA_SIM    â”‚
 â”‚      âš ï¸ NEEDS: dataanalysis.tf_idf_vectorize, cosine_sim   â”‚
 â”‚   4. build_content_sim_mh_text(network, mh_sigs)            â”‚
 â”‚      â†’ datasketch MinHashLSH(0.7, 512) â†’ CONTENT_SIM       â”‚
 â”‚   5. build_content_sim_num_overlap(network, num_sigs)       â”‚
 â”‚      â†’ medianÂ±IQR overlap â‰¥ 0.85 â†’ CONTENT_SIM             â”‚
 â”‚      â†’ core overlap â‰¥ 0.3 â†’ INCLUSION_DEPENDENCY            â”‚
 â”‚      â†’ DBSCAN(eps=0.1) for single-point columns             â”‚
 â”‚   6. build_pkfk(network, store)                             â”‚
 â”‚      â†’ cardinality ratio > 0.7 â†’ PKFK edges                â”‚
 â”‚   7. serialize_network(path) â†’ pickle files                  â”‚
 â”‚                                                             â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ STAGE 2: SYSTEM INIT  (âœ… aurum_v2/discovery/api.py)        â”‚
 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
 â”‚                                                             â”‚
 â”‚  init_system(config):                                       â”‚
 â”‚   â†’ network = deserialize_network(path)                     â”‚
 â”‚   â†’ store   = StoreHandler(config)                          â”‚
 â”‚   â†’ api     = API(network, store)  [API extends Algebra]    â”‚
 â”‚                                                             â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ STAGE 3: QUERY  (âœ… aurum_v2/discovery/algebra.py)          â”‚
 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
 â”‚                                                             â”‚
 â”‚  Algebra operations:                                        â”‚
 â”‚   search:   keyword_search(kw, type) â†’ DRS                 â”‚
 â”‚   navigate: neighbor_search(drs, rel) â†’ DRS                â”‚
 â”‚   paths:    find_path(a, b, rel) â†’ DRS (w/ provenance)     â”‚
 â”‚   set ops:  intersection, union, difference                 â”‚
 â”‚   ranking:  rank_certainty, rank_coverage                   â”‚
 â”‚   prov:     drs.why(hit), drs.how(hit)                     â”‚
 â”‚                                                             â”‚
 â”‚  âš ï¸ MISSING: ~20 API convenience wrappers (search_content, â”‚
 â”‚              similar_schema_to, pkfk_of, etc.)              â”‚
 â”‚                                                             â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ STAGE 4: DATA ON DEMAND  (âœ… aurum_v2/dod/)                 â”‚
 â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
 â”‚                                                             â”‚
 â”‚  dod.virtual_schema_iterative_search(attrs, values):        â”‚
 â”‚   1. Search filters â†’ DRS per attribute/value               â”‚
 â”‚   2. Group tables by filter coverage â†’ candidate groups      â”‚
 â”‚   3. Join graph discovery â†’ api.find_path(t1,t2,PKFK)      â”‚
 â”‚   4. is_join_graph_materializable() â†’ trial joins           â”‚
 â”‚   5. materialize_join_graph() â†’ tree-fold join pipeline     â”‚
 â”‚      â†’ join_ab_on_key_optimizer (3-min timeout, chunked)    â”‚
 â”‚      â†’ project requested columns                            â”‚
 â”‚      â†’ yield (materialized_view, metadata)                  â”‚
 â”‚                                                             â”‚
 â”‚  âš ï¸ MISSING: view_analysis.py â€” ViewClass 4C classification â”‚
 â”‚  âš ï¸ MISSING: 7 join utility functions                       â”‚
 â”‚                                                             â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7  Action Items for aurum_v2 Completion

### Priority 1 â€” Required for Functional System

| # | Action | Files to Create/Modify |
|---|--------|----------------------|
| 1 | **Add Python profiler module** | `aurum_v2/profiler/__init__.py`, `profiler/column_profiler.py`, `profiler/source_readers.py` |
| 2 | **Add data analysis module** | `aurum_v2/builder/analysis.py` â€” TF-IDF vectorize, cosine sim, overlap functions |
| 3 | **Add annotation/metadata types** | `aurum_v2/models/annotation.py` â€” MDClass, MDRelation, MDHit, MDComment, MRS |
| 4 | **Add view analysis module** | `aurum_v2/dod/view_analysis.py` â€” ViewClass enum + 4C classification functions |

### Priority 2 â€” Required for Feature Parity

| # | Action | Files to Modify |
|---|--------|----------------|
| 5 | **Add API convenience methods** | `aurum_v2/discovery/api.py` â€” ~20 user-facing wrappers |
| 6 | **Add missing store methods** | `aurum_v2/store/elastic_store.py` â€” annotation CRUD, fuzzy search, etc. |
| 7 | **Add missing join functions** | `aurum_v2/dod/join_utils.py` â€” disk join, NaN-safe join, selectivity |
| 8 | **Add missing text utils** | `aurum_v2/utils/text_utils.py` â€” POS tagging, lemmatization |
| 9 | **Add reporting module** | `aurum_v2/graph/reporting.py` â€” graph statistics |

### Priority 3 â€” Optional Enhancements

| # | Action | Notes |
|---|--------|-------|
| 10 | Alt. builder algorithms (LSA, SVD) | Legacy had experimental variants; not needed initially |
| 11 | Interactive sugar module | Jupyter convenience; can be added when needed |
| 12 | Neo4j export script | One-way visualization tool; optional |
