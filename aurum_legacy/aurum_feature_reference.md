# Aurum Feature Reference â€” Debugging Cross-Reference

> **Purpose:** Per-file inventory of every feature in the legacy Aurum codebase.  
> Use this while debugging the new **LakeAgent** project.  
>
> **Legend:**
> - ðŸ”´ **IMPORTANT** â€” Core algorithm / ported to LakeAgent / you will likely need to cross-reference this when debugging.
> - ðŸŸ¡ **MODERATE** â€” Supporting logic that affects behavior but is not a core algorithm.
> - âšª **NOT IMPORTANT** â€” Legacy infrastructure, deprecated, test-only, or debug-only code.

---

## Table of Contents

1. [api/apiutils.py](#apiapiutilspy) â€” Hit, DRS, Provenance, Relation, OP
2. [knowledgerepr/fieldnetwork.py](#knowledgerepfieldnetworkpy) â€” FieldNetwork graph wrapper
3. [knowledgerepr/networkbuilder.py](#knowledgerepnetworkbuilderpy) â€” Edge-building algorithms
4. [networkbuildercoordinator.py](#networkbuildercoordinatorpy) â€” Pipeline orchestrator
5. [modelstore/elasticstore.py](#modelstoreelasticstorepy) â€” Elasticsearch client
6. [algebra.py](#algebrapy) â€” Newer query algebra API
7. [ddapi.py](#ddapipy) â€” Older query API
8. [DoD/dod.py](#doddodpy) â€” Data-on-Demand view search
9. [DoD/data_processing_utils.py](#doddata_processing_utilspy) â€” Join engine & CSV I/O
10. [DoD/material_view_analysis.py](#dodmaterial_view_analysispy) â€” View comparison
11. [DoD/utils.py](#dodutilspy) â€” FilterType enum
12. [DoD/experimental.py](#dodexperimentalpy) â€” Exhaustive search variant
13. [dataanalysis/dataanalysis.py](#dataanalysisdataanalysispy) â€” Column comparison analytics
14. [dataanalysis/nlp_utils.py](#dataanalysisnlp_utilspy) â€” Text preprocessing
15. [config.py](#configpy) â€” Global configuration
16. [ontomatch/ss_api.py](#ontomatchss_apipy) â€” Semantic schema matching API
17. [ontomatch/ss_utils.py](#ontomatchss_utilspy) â€” Semantic similarity utilities
18. [ontomatch/matcher_lib.py](#ontomatchmatcher_libpy) â€” Matching library
19. [ontomatch/glove_api.py](#ontomatchglove_apipy) â€” GloVe embeddings
20. [ontomatch/onto_parser.py](#ontomatchonto_parserpy) â€” Ontology parser
21. [ontomatch/no_matcher.py](#ontomatchno_matcherpy) â€” Wikipedia text matcher
22. [knowledgerepr/lite_graph.py](#knowledgereprlite_graphpy) â€” Bitarray graph
23. [inputoutput/inputoutput.py](#inputoutputinputoutputpy) â€” Pickle serialization
24. [api/annotation.py](#apiannotationpy) â€” Metadata annotation types
25. [api/reporting.py](#apireportingpy) â€” Network statistics
26. [sugar.py](#sugarpy) â€” Convenience REPL shortcuts
27. [main.py](#mainpy) â€” System init & IPython shell
28. [run_dod.py](#run_dodpy) â€” DoD CLI entry point
29. [server_config.py](#server_configpy) â€” Server path config
30. [server-api/app.py](#server-apiapppy) â€” Flask web API
31. [aurum_cli.py](#aurum_clipy) â€” Fire CLI wrapper
32. [export_network_2_neo4j.py](#export_network_2_neo4jpy) â€” Neo4j exporter

---

## api/apiutils.py
*904 lines â€” Core data structures used everywhere*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `compute_field_id(db, src, field)` | ~15 | ðŸ”´ **IMPORTANT** | `result_set.py` | Uses `binascii.crc32` to produce integer IDs. Your new code may use a different hash; if IDs mismatch, nothing links. |
| 2 | `Hit` namedtuple | ~25 | ðŸ”´ **IMPORTANT** | `result_set.py` | `(nid, db_name, source_name, field_name, score)` â€” hash on `int(nid)`. Custom `__hash__` and `__eq__` drive all set operations and graph lookups. |
| 3 | `Relation` enum | ~50 | ðŸ”´ **IMPORTANT** | `graph/relations.py` | SCHEMA=0, SCHEMA_SIM=1, CONTENT_SIM=2, ENTITY_SIM=3, PKFK=5, INCLUSION_DEPENDENCY=6, plus metadata relations 10-15. `.from_metadata()` maps ints. |
| 4 | `OP` enum | ~70 | ðŸŸ¡ MODERATE | `result_set.py` | Operation provenance codes. Mirrors `Relation` values. Used in provenance graph edges. |
| 5 | `DRSMode` enum | ~80 | ðŸŸ¡ MODERATE | `result_set.py` | FIELDS=0, TABLE=1. Controls iteration behavior of DRS. |
| 6 | `Operation` class | ~85 | ðŸŸ¡ MODERATE | â€” | Wraps `op` + `params` for provenance tracking. |
| 7 | `Provenance` class | ~90-200 | ðŸŸ¡ MODERATE | Simplified in LakeAgent | nx.MultiDiGraph-based DAG. Methods: `populate_provenance()`, `get_leafs_and_heads()`, `compute_paths_from_origin_to()`, `compute_all_paths()`, `compute_paths_with()`, `explain_path()`. |
| 8 | `DRS` class â€” core container | ~200-400 | ðŸ”´ **IMPORTANT** | `discovery/result_set.py` | The main result object. Holds `self.data` (set of Hits), provenance graph, mode (field/table). Iterator switches between field-level and table-level output. |
| 9 | `DRS.absorb_provenance()` | ~350 | ðŸŸ¡ MODERATE | â€” | Merges provenance graphs via `nx.compose()` with AND/OR edge annotations. |
| 10 | `DRS.absorb()` | ~370 | ðŸŸ¡ MODERATE | â€” | Set union + provenance merge. |
| 11 | `DRS.intersection()` / `union()` / `set_difference()` | ~400-450 | ðŸ”´ **IMPORTANT** | `result_set.py` | Set algebra on result sets. These drive the compositional query model. |
| 12 | `DRS.paths()` / `path(a)` | ~460 | ðŸŸ¡ MODERATE | â€” | Provenance path enumeration. |
| 13 | `DRS.why(a)` / `how(a)` | ~500 | âšª NOT IMPORTANT | â€” | Provenance explanation â€” nice-to-have, not core. |
| 14 | `DRS._compute_certainty_scores()` | ~600 | ðŸŸ¡ MODERATE | â€” | Recursive graph traversal to compute certainty ranking scores. |
| 15 | `DRS._compute_coverage_scores()` | ~650 | ðŸŸ¡ MODERATE | â€” | Bitarray-based coverage scoring. |
| 16 | `DRS.rank_certainty()` / `rank_coverage()` | ~700 | ðŸŸ¡ MODERATE | â€” | Sort data by scores. |
| 17 | `DRS.print_tables()` / `print_columns()` / `pretty_print_columns()` | ~750 | âšª NOT IMPORTANT | â€” | Display helpers. |
| 18 | `DRS.__dict__()` | ~850 | âšª NOT IMPORTANT | â€” | JSON serialization for Flask web API. |

---

## knowledgerepr/fieldnetwork.py
*482 lines â€” Central graph that stores the knowledge network*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `FieldNetwork.__init__(G, id_to_info, table_to_ids)` | ~30 | ðŸ”´ **IMPORTANT** | `graph/field_network.py` | Wraps a `nx.MultiGraph` + two dicts. Everything queries through this. |
| 2 | `init_meta_schema(self, store)` | ~50 | ðŸ”´ **IMPORTANT** | `graph/network_builder.py` | Populates graph nodes from ES (all fields with cardinality). This is the schema skeleton before edges. |
| 3 | `add_field()` / `add_fields()` / `add_relation()` | ~80 | ðŸ”´ **IMPORTANT** | `field_network.py` | Graph mutation primitives. `add_relation` creates typed edges. |
| 4 | `iterate_ids()` / `iterate_ids_text()` / `iterate_values()` | ~120 | ðŸ”´ **IMPORTANT** | `field_network.py` | Generators that yield `(db_name, source_name, field_name, data_type)` from graph nodes. Used everywhere during edge building. |
| 5 | `neighbors_id(hit, relation)` â†’ DRS | ~180 | ðŸ”´ **IMPORTANT** | `field_network.py` | Core traversal: given a Hit and relation type, return all neighbor Hits as a DRS. This is the fundamental graph query. |
| 6 | `md_neighbors_id()` | ~220 | âšª NOT IMPORTANT | â€” | Metadata-relation neighbor traversal. Only used with annotation system. |
| 7 | `find_path_hit(source, target, relation, max_hops)` | ~250 | ðŸ”´ **IMPORTANT** | `field_network.py` | DFS path finding between two Hits with provenance assembly. Core to `paths()` in algebra. |
| 8 | `find_path_table(source, target, relation, max_hops)` | ~300 | ðŸ”´ **IMPORTANT** | `field_network.py` | Table-level DFS with sibling tracking. Complex provenance assembly with `sources` list tracking same-table attributes. |
| 9 | `enumerate_relation(relation)` | ~380 | ðŸŸ¡ MODERATE | â€” | Yields all edges of a given relation type. Used in reporting/stats. |
| 10 | `get_op_from_relation()` | ~400 | ðŸŸ¡ MODERATE | â€” | Maps Relation enum â†’ OP enum for provenance. |
| 11 | `fields_degree(topk)` | ~420 | âšª NOT IMPORTANT | â€” | Returns top-k nodes by degree. Diagnostic only. |
| 12 | `serialize_network()` / `deserialize_network()` | ~450 | ðŸŸ¡ MODERATE | `field_network.py` | Pickle-based serde via `nx.write_gpickle`/`nx.read_gpickle`. Your new code replaces this. |
| 13 | `serialize_network_to_csv()` | ~440 | âšª NOT IMPORTANT | â€” | Debug CSV export. |

---

## knowledgerepr/networkbuilder.py
*672 lines â€” The algorithms that create graph edges (the "secret sauce")*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `LSHRandomProjectionsIndex` class | ~30 | ðŸ”´ **IMPORTANT** | `graph/network_builder.py` (uses datasketch instead) | Wraps NearPy `RandomBinaryProjections`. Your code uses datasketch MinHashLSH insteadâ€”same concept, different library. |
| 2 | `build_schema_sim_relation(network, store)` | ~80 | ðŸ”´ **IMPORTANT** | `network_builder.py::_build_schema_sim()` | TF-IDF on column names â†’ LSH â†’ SCHEMA_SIM edges. **Key thresholds:** uses NearPy LSH with RandomBinaryProjections (10 bits). |
| 3 | `build_schema_sim_relation_lsa()` | ~120 | âšª NOT IMPORTANT | â€” | LSA variant. Not called in coordinator. Dead code. |
| 4 | `build_entity_sim_relation()` | ~160 | âšª NOT IMPORTANT | â€” | TF-IDF on entities â†’ ENTITY_SIM. Commented out in coordinator. |
| 5 | `build_content_sim_relation_text()` | ~200 | âšª NOT IMPORTANT | â€” | TF-IDF+LSH on text values. Superseded by MinHash variant. |
| 6 | `build_content_sim_mh_text(network, store)` | ~280 | ðŸ”´ **IMPORTANT** | `network_builder.py::_build_content_sim_text()` | **MinHash LSH** on text column values. `threshold=0.7`, `num_perm=512`. Retrieves pre-computed minhash arrays from ES, queries LSH index, creates CONTENT_SIM edges. This is the active text similarity method. |
| 7 | `build_content_sim_relation_num_overlap_distr(network, store)` | ~340 | ðŸ”´ **IMPORTANT** | `network_builder.py::_build_content_sim_numeric()` | **Numeric overlap detection.** Uses median Â± IQR overlap (threshold=0.85) as primary check, then inclusion dependency (threshold=0.3), plus DBSCAN (eps=0.1) for single-point clusters. Creates CONTENT_SIM edges for numeric columns. |
| 8 | `build_content_sim_relation_num_overlap_distr_indexed()` | ~430 | âšª NOT IMPORTANT | â€” | Event-sweep variant. Incomplete/unused. |
| 9 | `build_content_sim_relation_num_double_clustering()` | ~480 | âšª NOT IMPORTANT | â€” | Experimental DBSCAN on median AND IQR. Not used. |
| 10 | `build_content_sim_relation_num()` | ~530 | âšª NOT IMPORTANT | â€” | Deprecated DBSCAN on raw features. |
| 11 | `build_pkfk_relation(network, store)` | ~580 | ðŸ”´ **IMPORTANT** | `network_builder.py::_build_pkfk()` | **PK/FK detection.** Cardinality ratio > 0.7, plus neighbor cross-check. Creates PKFK edges. |
| 12 | `index_in_text_engine()` | ~50 | ðŸŸ¡ MODERATE | â€” | Indexes TF-IDF vectors into NearPy engine. Internal to schema_sim. |
| 13 | `create_sim_graph_text()` | ~60 | ðŸŸ¡ MODERATE | â€” | NearPy-based LSH neighbor search loop. Internal to edge builders. |
| 14 | `lsa_dimensionality_reduction()` | ~70 | âšª NOT IMPORTANT | â€” | TruncatedSVD to 1000 components. Only used in dead LSA paths. |

---

## networkbuildercoordinator.py
*~200 lines â€” Orchestrates the full network building pipeline*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `main(output_path)` | ~30 | ðŸ”´ **IMPORTANT** | `network_builder.py::NetworkBuilder.build()` | Full pipeline: (1) init_meta_schema, (2) build_schema_sim, (3) entity_sim [commented out], (4) build_content_sim_mh_text, (5) build_content_sim_num_overlap_distr, (6) build_pkfk. Then serializes. The ordering and which functions are called is critical. |
| 2 | `plot_num()` | ~150 | âšª NOT IMPORTANT | â€” | Debug matplotlib visualization. |
| 3 | `test_content_sim_num()` | ~170 | âšª NOT IMPORTANT | â€” | Test harness for numeric similarity. |

---

## modelstore/elasticstore.py
*789 lines â€” Elasticsearch interface for all profile data*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `StoreHandler.__init__()` | ~30 | ðŸŸ¡ MODERATE | LakeAgent uses Polars directly, no ES | Connects to ES at `localhost:9200`. |
| 2 | `KWType` enum | ~20 | ðŸ”´ **IMPORTANT** | â€” | KW_CONTENT, KW_SCHEMA, KW_ENTITIES, KW_TABLE, KW_METADATA. Controls search scope. |
| 3 | `get_all_fields()` | ~80 | ðŸ”´ **IMPORTANT** | `profiler/column_profiler.py` | Scrolls all ES profile docs â†’ `(id, dbName, sourceName, columnName, totalValues, uniqueValues, dataType)`. This populates the graph skeleton. |
| 4 | `search_keywords(kw, kw_type, max_hits)` | ~120 | ðŸ”´ **IMPORTANT** | `algebra.py::search()` | ES match query on text/profile/entities indices. Returns Hits. |
| 5 | `exact_search_keywords()` | ~160 | ðŸ”´ **IMPORTANT** | `algebra.py::exact_search()` | Term query (exact match). |
| 6 | `fuzzy_keyword_match()` | ~190 | ðŸŸ¡ MODERATE | â€” | Fuzzy match on text index. |
| 7 | `suggest_schema()` | ~210 | ðŸŸ¡ MODERATE | â€” | ES completion suggester on `columnNameSuggest`. |
| 8 | `get_all_fields_text_signatures()` | ~250 | ðŸ”´ **IMPORTANT** | `column_profiler.py` | Retrieves term vectors via ES `mtermvectors` API, filters by frequency (>3) and length (>3 chars). Used by TF-IDF edge builders. |
| 9 | `get_all_mh_text_signatures()` | ~350 | ðŸ”´ **IMPORTANT** | `column_profiler.py` | Retrieves pre-computed minhash arrays for text columns. Used by `build_content_sim_mh_text`. |
| 10 | `get_all_fields_num_signatures()` | ~400 | ðŸ”´ **IMPORTANT** | `column_profiler.py` | Retrieves `(median, iqr, minValue, maxValue)` for numeric columns. Used by numeric overlap builder. |
| 11 | `get_path_of(nid)` | ~70 | ðŸŸ¡ MODERATE | â€” | Retrieves filesystem path for a data source given its nid. Used in DoD materialization. |
| 12 | `add_annotation()` / `add_comment()` / `add_tags()` | ~500 | âšª NOT IMPORTANT | â€” | Metadata CRUD. Annotation system. |
| 13 | `search_keywords_md()` / `get_metadata()` / `get_comments()` | ~550 | âšª NOT IMPORTANT | â€” | Metadata search. |
| 14 | `create_metadata_index()` / `delete_metadata_index()` | ~600 | âšª NOT IMPORTANT | â€” | ES index lifecycle management. |

---

## algebra.py
*614 lines â€” Newer compositional query API*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `Algebra.__init__(network, store_client)` | ~30 | ðŸ”´ **IMPORTANT** | `discovery/algebra.py` | Takes the FieldNetwork + StoreHandler. |
| 2 | `search(kw, kw_type, max_results)` | ~50 | ðŸ”´ **IMPORTANT** | `algebra.py::search()` | ES keyword search â†’ DRS. |
| 3 | `exact_search()` | ~70 | ðŸ”´ **IMPORTANT** | `algebra.py::exact_search()` | ES exact match â†’ DRS. |
| 4 | `search_content()` / `search_attribute()` / `search_exact_attribute()` / `search_table()` | ~80-120 | ðŸ”´ **IMPORTANT** | `algebra.py` | Convenience wrappers that set `KWType`. |
| 5 | `suggest_schema()` | ~130 | ðŸŸ¡ MODERATE | â€” | ES completion suggester wrapper. |
| 6 | `__neighbor_search(input, relation)` | ~150 | ðŸ”´ **IMPORTANT** | `algebra.py::neighbor_search()` | Core traversal: converts any input â†’ DRS, iterates hits, gets neighbors by relation from FieldNetwork. Central to all similarity queries. |
| 7 | `content_similar_to()` / `schema_similar_to()` / `pkfk_of()` | ~200 | ðŸ”´ **IMPORTANT** | `algebra.py` | Convenience wrappers for `__neighbor_search` with specific Relation types. |
| 8 | `paths(drs_a, drs_b, relation, max_hops, lean_search)` | ~250 | ðŸ”´ **IMPORTANT** | `algebra.py::paths()` | Path finding between two DRS. Dispatches to `find_path_hit` or `find_path_table` depending on mode. `max_hops` default is 3. |
| 9 | `__traverse(a, primitive, max_hops)` | ~300 | ðŸŸ¡ MODERATE | â€” | BFS traversal up to max_hops using a given primitive (e.g., `content_similar_to`). |
| 10 | `intersection()` / `union()` / `difference()` | ~350 | ðŸ”´ **IMPORTANT** | `algebra.py` | Set algebra on DRS. These compose queries. |
| 11 | `make_drs(general_input)` / `_general_to_drs()` | ~400 | ðŸ”´ **IMPORTANT** | `algebra.py` | Converts int/str/tuple/Hit/DRS â†’ DRS. Handles table name lookup, nid lookup, etc. |
| 12 | `_hit_to_drs()` / `drs_from_table_hit()` | ~450 | ðŸŸ¡ MODERATE | â€” | Expands a Hit to include all sibling columns from same table. |
| 13 | Metadata API (`annotate`, `add_comments`, `add_tags`, `md_search`) | ~500 | âšª NOT IMPORTANT | â€” | Hidden with `__` prefix. Annotation system. |
| 14 | `Helper` class | ~550 | âšª NOT IMPORTANT | â€” | `reverse_lookup()`, `get_path_nid()`, `help()`. Convenience. |
| 15 | `API(Algebra)` subclass | ~600 | âšª NOT IMPORTANT | â€” | Just a subclass alias. |

---

## ddapi.py
*620 lines â€” Original/older query API*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `DDAPI.__init__(network)` | ~30 | ðŸŸ¡ MODERATE | â€” | Older API that takes only network (no store_client at init). |
| 2 | Seed methods: `drs_from_raw_field()`, `drs_from_hit()`, `drs_from_table()` | ~50-100 | ðŸŸ¡ MODERATE | â€” | Create initial DRS from raw inputs. Superseded by `algebra.py::make_drs()`. |
| 3 | `keyword_search()` / `schema_name_search()` / `entity_search()` | ~120-200 | ðŸŸ¡ MODERATE | â€” | Search primitives. Duplicate of algebra.py functionality. |
| 4 | `similar_schema_name_to()` / `similar_content_to()` / `pkfk_of()` | ~250-300 | ðŸŸ¡ MODERATE | â€” | Neighbor search wrappers. Same as algebra.py. |
| 5 | `paths_between()` / `paths()` / `traverse()` | ~350-400 | ðŸŸ¡ MODERATE | â€” | Path finding. Same logic as algebra.py. |
| 6 | `intersection()` / `union()` / `difference()` | ~420-460 | ðŸŸ¡ MODERATE | â€” | Set algebra. Same as algebra.py. |
| 7 | `ResultFormatter.format_output_for_webclient()` | ~500 | âšª NOT IMPORTANT | â€” | HTML/JSON formatting for web UI. |
| 8 | `API(DDAPI)` subclass + `init_store()` | ~580 | âšª NOT IMPORTANT | â€” | Creates StoreHandler lazily. Legacy pattern. |

---

## DoD/dod.py
*958 lines â€” Data-on-Demand: the view search pipeline*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `DoD.__init__(network, store_client, csv_separator)` | ~30 | ðŸ”´ **IMPORTANT** | `discovery/data_on_demand.py` | Creates an internal `API` and configures paths_cache. |
| 2 | `individual_filters(list_attributes, list_samples)` | ~60 | ðŸ”´ **IMPORTANT** | `data_on_demand.py` | Searches for each attr and cell value separately via the algebra API. Returns `{(filter, FilterType): DRS}`. |
| 3 | `joint_filters(list_attributes, list_samples)` | ~100 | ðŸ”´ **IMPORTANT** | `data_on_demand.py` | Combined attr+cell search with intersection on same-column pairs. |
| 4 | `virtual_schema_iterative_search(list_attributes, list_samples)` | ~150 | ðŸ”´ **IMPORTANT** | `data_on_demand.py::search_views()` | **MAIN PIPELINE.** 5 stages: (1) `joint_filters` â†’ search, (2) `eager_candidate_exploration` â†’ greedy set cover, (3) `joinable` â†’ find join paths, (4) `is_join_graph_materializable` â†’ validate, (5) `materialize_join_graphs` â†’ yield DataFrames. **This is the heart of DoD.** |
| 5 | `eager_candidate_exploration()` (inside virtual_schema) | ~200 | ðŸ”´ **IMPORTANT** | `data_on_demand.py` | Greedy set cover: sorts tables by filter coverage, picks tables eagerly, yields candidate groups covering all filters. Uses nested generators. |
| 6 | `joinable(candidate_group)` | ~350 | ðŸ”´ **IMPORTANT** | `data_on_demand.py` | For each pair of tables in group, finds all PKFK paths via `api.paths()`. Uses `itertools.product` to enumerate join graphs. Deduplicates via `compute_join_graph_id()`. |
| 7 | `transform_join_path_to_pair_hop()` | ~450 | ðŸŸ¡ MODERATE | â€” | Converts a path list to `[(left, right)]` pairs, removing same-table hops. |
| 8 | `compute_join_graph_id()` | ~470 | ðŸŸ¡ MODERATE | â€” | Hash-based deduplication of join graphs. |
| 9 | `format_join_graph_into_nodes_edges()` | ~490 | ðŸŸ¡ MODERATE | â€” | Converts join graph to JSON-ready `{nodes, edges}` format for the UI. |
| 10 | `is_join_graph_materializable(join_graph, filters)` | ~520 | ðŸ”´ **IMPORTANT** | `data_on_demand.py` | Validation: applies filters to each table, attempts joins hop-by-hop, verifies cardinality > 0. If any hop produces empty result, the graph is rejected. |
| 11 | `rank_materializable_join_graphs()` | ~700 | ðŸŸ¡ MODERATE | â€” | Scores join graphs by key likelihood (uniqueness ratio). |
| 12 | `obtain_table_paths()` | ~750 | ðŸŸ¡ MODERATE | â€” | Gets filesystem paths for table sources via `store_client.get_path_of()`. |
| 13 | `test_e2e()` | ~800 | âšª NOT IMPORTANT | â€” | End-to-end test harness. |

---

## DoD/data_processing_utils.py
*736 lines â€” Join execution engine and data I/O*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `read_relation(path, separator)` / `read_relation_on_copy()` | ~30 | ðŸ”´ **IMPORTANT** | `materialization/join_engine.py` | Cached CSV reading with `pd.read_csv`. `read_relation_on_copy` returns `df.copy()` to avoid mutation. LakeAgent uses Polars instead. |
| 2 | `apply_filter(relation, attribute, cell_value)` | ~80 | ðŸ”´ **IMPORTANT** | `join_engine.py` | Filters rows where `attribute == cell_value` (case-insensitive string comparison). |
| 3 | `find_key_for(relation, attribute, value)` | ~100 | ðŸŸ¡ MODERATE | â€” | `SELECT key FROM relation WHERE attribute = value`. |
| 4 | `is_value_in_column(relation, attribute, value)` | ~120 | ðŸŸ¡ MODERATE | â€” | Boolean existence check. |
| 5 | `obtain_attributes_to_project(filters)` | ~140 | ðŸŸ¡ MODERATE | â€” | Extracts ATTR-type filter names for final column projection. |
| 6 | `project(relation, attrs)` | ~160 | ðŸŸ¡ MODERATE | â€” | Column projection (select columns). |
| 7 | `estimate_output_row_size()` | ~180 | ðŸŸ¡ MODERATE | â€” | Bytes-per-row estimation for memory planning. |
| 8 | `does_join_fit_in_memory()` | ~200 | ðŸŸ¡ MODERATE | â€” | Checks against `memory_limit_join_processing` (60% of RAM via `psutil`). |
| 9 | `join_ab_on_key(a, b, key_a, key_b)` | ~220 | ðŸ”´ **IMPORTANT** | `join_engine.py::join()` | Simple `pd.merge(a, b, left_on=key_a, right_on=key_b, how='inner')`. The basic join primitive. |
| 10 | `join_ab_on_key_optimizer(a, b, key_a, key_b)` | ~250 | ðŸ”´ **IMPORTANT** | `join_engine.py::join()` | **Memory-aware chunked join.** Normalizes keys to lowercase strings, drops NaN/null, shuffles b for uniform sampling, first-chunk memory estimation, **3-minute timeout**, disk-spill fallback. This is the production join. |
| 11 | `join_ab_on_key_spill_disk()` | ~380 | ðŸŸ¡ MODERATE | â€” | Always-spill variant. Writes to temp files. |
| 12 | `InTreeNode` class | ~420 | ðŸŸ¡ MODERATE | â€” | Tree node for join materialization. Has `relation`, `key`, `children`. |
| 13 | `materialize_join_graph(join_graph, filters)` | ~450 | ðŸ”´ **IMPORTANT** | `join_engine.py::materialize()` | Builds an in-tree from join graph edges, applies filters to leaves, then folds leaves upward via joins. The tree-fold is the core materialization strategy. |
| 14 | `apply_consistent_sample()` | ~550 | ðŸŸ¡ MODERATE | â€” | Deterministic sampling by hash-sorting IDs. Used for sampled materialization. |
| 15 | `materialize_join_graph_sample()` | ~580 | ðŸŸ¡ MODERATE | â€” | Sampled version of `materialize_join_graph`. |
| 16 | `estimate_join_memory()` | ~650 | ðŸŸ¡ MODERATE | â€” | Cartesian product size estimation for memory checking. |

---

## DoD/material_view_analysis.py
*204 lines â€” Comparing materialized views*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `EQUI` enum | ~10 | ðŸŸ¡ MODERATE | `materialization/view_analysis.py` | EQUIVALENT, DIF_CARDINALITY, DIF_SCHEMA, DIF_VALUES. |
| 2 | `most_likely_key(view)` | ~20 | ðŸ”´ **IMPORTANT** | `view_analysis.py` | Column with highest `unique/total` ratio. Used for join ranking. |
| 3 | `uniqueness(view)` | ~40 | ðŸŸ¡ MODERATE | `view_analysis.py` | Per-column uniqueness ratio dictionary. |
| 4 | `curate_view(view)` | ~60 | ðŸŸ¡ MODERATE | â€” | Drop NaN, deduplicate, reset+sort indices. Cleanup before comparison. |
| 5 | `equivalent(v1, v2)` | ~80 | ðŸŸ¡ MODERATE | `view_analysis.py` | Same cardinality + same schema + same values (case-insensitive, sorted). |
| 6 | `contained(v1, v2)` | ~110 | ðŸŸ¡ MODERATE | `view_analysis.py` | Every value in smaller set exists in larger. |
| 7 | `complementary(v1, v2)` | ~130 | ðŸŸ¡ MODERATE | `view_analysis.py` | Key sets have non-empty symmetric difference. |
| 8 | `contradictory(v1, v2)` | ~150 | ðŸŸ¡ MODERATE | `view_analysis.py` | Same key values but different non-key values (groupby comparison). |
| 9 | `inconsistent_value_on_key(view)` | ~180 | ðŸŸ¡ MODERATE | â€” | Row-level conflict detection within a single view. |

---

## DoD/utils.py
*7 lines â€” Tiny enum*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `FilterType` enum | 1-7 | ðŸ”´ **IMPORTANT** | `data_on_demand.py` | CELL=0, ATTR=1. Used throughout DoD to distinguish between content and attribute filters. |

---

## DoD/experimental.py
*~70 lines â€” Exhaustive search variant*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `virtual_schema_exhaustive_search()` | all | âšª NOT IMPORTANT | â€” | Brute-force set cover using `itertools.combinations`. Not used in production; the iterative search in dod.py is the active algorithm. |

---

## dataanalysis/dataanalysis.py
*~400 lines â€” Statistical column comparison*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `compute_overlap(a, b)` | ~30 | ðŸ”´ **IMPORTANT** | `profiler/column_profiler.py` | Jaccard-like overlap with early termination. Uses `config.join_overlap_th = 0.4`. |
| 2 | `compute_overlap_of_columns(a, b)` | ~50 | ðŸ”´ **IMPORTANT** | â€” | Wrapper that reads columns from CSV, builds value dicts, calls `compute_overlap`. |
| 3 | `get_tfidf_docs(corpus)` | ~80 | ðŸŸ¡ MODERATE | â€” | Global `TfidfVectorizer(sublinear_tf=True, use_idf=True)`. Used by schema_sim edge builder. |
| 4 | `compare_pair_num_columns()` | ~100 | ðŸŸ¡ MODERATE | â€” | KS 2-sample test for numeric column comparison. |
| 5 | `compare_pair_text_columns()` | ~120 | ðŸŸ¡ MODERATE | â€” | TF-IDF cosine similarity for text column comparison. |
| 6 | `compare_num_columns_dist_ks()` | ~140 | ðŸŸ¡ MODERATE | â€” | `scipy.stats.ks_2samp` wrapper. |
| 7 | `compare_num_columns_dist_odsvm()` | ~160 | âšª NOT IMPORTANT | â€” | One-class SVM prediction. Experimental. |
| 8 | `get_numerical_signature()` | ~180 | ðŸŸ¡ MODERATE | â€” | KDE sampling for column signatures. |
| 9 | `get_textual_signature()` | ~200 | ðŸŸ¡ MODERATE | â€” | `CountVectorizer` top-5 terms. |
| 10 | `get_sim_matrix_numerical()` / `get_sim_matrix_text()` | ~250 | âšª NOT IMPORTANT | â€” | Full pairwise comparison matrices. Not used in pipeline. |
| 11 | `build_dict_values()` | ~20 | ðŸŸ¡ MODERATE | â€” | Value frequency counter for overlap computation. |

---

## dataanalysis/nlp_utils.py
*~55 lines â€” Text preprocessing utilities*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `camelcase_to_snakecase(term)` | ~30 | ðŸ”´ **IMPORTANT** | `profiler/text_utils.py` | Regex-based CamelCase â†’ snake_case. Used in ALL name tokenization across the codebase. |
| 2 | `tokenize_property(prop)` | ~35 | ðŸŸ¡ MODERATE | `text_utils.py` | snake_case + split on `_` and `-`. |
| 3 | `curate_tokens(tokens)` | ~40 | ðŸŸ¡ MODERATE | `text_utils.py` | Lowercase, remove stopwords, remove lenâ‰¤1, deduplicate. |
| 4 | `curate_string(string)` | ~45 | ðŸŸ¡ MODERATE | `text_utils.py` | CamelCaseâ†’snake, replace `_`/`-` with spaces, lowercase. |
| 5 | `pos_tag_text()` / `get_nouns()` / `get_proper_nouns()` | ~10-25 | âšª NOT IMPORTANT | â€” | NLTK POS tagging. Only used in ontomatch `bow_repr_of`. |

---

## config.py
*~30 lines â€” Global configuration constants*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `join_overlap_th = 0.4` | ~5 | ðŸ”´ **IMPORTANT** | `config.py` | Threshold for Jaccard overlap in joins. |
| 2 | `k = 512` | ~6 | ðŸ”´ **IMPORTANT** | `config.py` | MinHash permutation count. |
| 3 | `separator = '\|'` | ~7 | ðŸŸ¡ MODERATE | `config.py` | Default CSV separator. |
| 4 | `join_chunksize = 1000` | ~8 | ðŸŸ¡ MODERATE | â€” | Chunk size for chunked joins. |
| 5 | `memory_limit_join_processing = 0.6` | ~9 | ðŸŸ¡ MODERATE | â€” | 60% of RAM limit. |
| 6 | Serde paths (graphfile, graphcachedfile, etc.) | ~15 | âšª NOT IMPORTANT | â€” | Legacy file paths. |
| 7 | `db_host = 'localhost'`, `db_port = '9200'` | ~25 | âšª NOT IMPORTANT | â€” | Elasticsearch connection. Not used in LakeAgent. |

---

## ontomatch/ss_api.py
*1956 lines â€” Semantic Schema Matching API*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `SSAPI.__init__(network, store, schema_sim_idx, content_sim_idx)` | ~25 | âšª NOT IMPORTANT | â€” | Ontology matching system. Not ported to LakeAgent. |
| 2 | `SSAPI.add_krs(kr_name_paths)` | ~45 | âšª NOT IMPORTANT | â€” | Registers ontologies (OWL files) for matching. |
| 3 | `SSAPI.find_matchings()` | ~100 | âšª NOT IMPORTANT | â€” | Multi-level matching pipeline: L1 (classâ†’content), L4 (relationâ†’class syntax), L5 (attrâ†’class syntax), L42 (semantic), L52 (semantic). Most levels are commented out. |
| 4 | `SSAPI.find_links(matchings)` | ~330 | âšª NOT IMPORTANT | â€” | Given matchings, discovers `is_a` links via ontology hierarchy. |
| 5 | `SSAPI.find_coarse_grain_hooks()` | ~480 | âšª NOT IMPORTANT | â€” | Deprecated. LSH-indexed semantic vector matching for tables to ontology classes. |
| 6 | `SSAPI.find_coarse_grain_hooks_n2()` | ~290 | âšª NOT IMPORTANT | â€” | O(nÂ²) variant of coarse grain hooks. |
| 7 | `test_l6()` / module-level test code | ~560+ | âšª NOT IMPORTANT | â€” | Test harnesses. |

---

## ontomatch/ss_utils.py
*589 lines â€” Semantic similarity computation utilities*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `minhash(str_values)` | ~25 | ðŸŸ¡ MODERATE | â€” | Java-compatible MinHash implementation. Uses custom hash matching the Java DDProfiler. Only needed if you must match Java-produced hashes. |
| 2 | `extract_cohesive_groups(table_name, attrs)` | ~70 | âšª NOT IMPORTANT | â€” | Groups semantically similar tokens using GloVe. Experimental. |
| 3 | `generate_table_vectors(path, network)` | ~170 | âšª NOT IMPORTANT | â€” | Creates GloVe-based semantic vectors per table. |
| 4 | `compute_semantic_similarity(sv1, sv2)` | ~260 | âšª NOT IMPORTANT | â€” | Core semantic similarity: pairwise GloVe dot products with penalization and signal strength. Only for ontomatch. |
| 5 | `compute_semantic_similarity_cross_average()` / `max_average()` / `min_average()` / `median()` | ~320-400 | âšª NOT IMPORTANT | â€” | Alternative aggregation strategies. Experimental. |
| 6 | `compute_internal_cohesion(sv)` | ~220 | âšª NOT IMPORTANT | â€” | Mean pairwise semantic distance within a vector set. |
| 7 | `store_signatures()` / `load_signatures()` | ~160 | âšª NOT IMPORTANT | â€” | Pickle serde for semantic vectors. |
| 8 | `read_table_columns(path, network)` | ~150 | âšª NOT IMPORTANT | â€” | Generator yielding `(db, table, [cols])` from FieldNetwork. |

---

## ontomatch/matcher_lib.py
*1660 lines â€” Matching algorithms library*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `MatchingType` enum | ~15 | âšª NOT IMPORTANT | â€” | L1 through L7 matching types. Only for ontomatch. |
| 2 | `SimpleTrie` class | ~30-130 | âšª NOT IMPORTANT | â€” | Trie for summarizing matchings to ancestor classes. |
| 3 | `Matching` class | ~140-200 | âšª NOT IMPORTANT | â€” | Accumulator for source-level and attr-level matchings. |
| 4 | `summarize_matchings_to_ancestor()` | ~240 | âšª NOT IMPORTANT | â€” | Uses trie to collapse matchings to ontology ancestor nodes. |
| 5 | `combine_matchings()` | ~400 | âšª NOT IMPORTANT | â€” | Merges all matching levels into `Matching` objects keyed by `(db, source)`. |
| 6 | `find_relation_class_name_matchings()` | â€” | âšª NOT IMPORTANT | â€” | L4: MinHash-based syntax matching between relation names and ontology class names. |
| 7 | `find_relation_class_attr_name_matching()` | â€” | âšª NOT IMPORTANT | â€” | L5: MinHash-based attr name â†” class name matching. |
| 8 | `find_relation_class_attr_name_sem_matchings()` | ~550 | âšª NOT IMPORTANT | â€” | L52: GloVe-based semantic matching between attribute names and class names. |
| 9 | `get_ban_indexes()` / `remove_banned_vectors()` | ~510 | âšª NOT IMPORTANT | â€” | Removes shared tokens before semantic comparison to avoid trivial matches. |
| 10 | `double_check_sem_signal_attr_sch_sch()` | ~200 | âšª NOT IMPORTANT | â€” | Re-checks semantic signal between two attributes. |

---

## ontomatch/glove_api.py
*~70 lines â€” GloVe word embedding loader*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `load_model(path)` / `load_vocab()` | ~35-60 | âšª NOT IMPORTANT | LakeAgent uses sentence-transformers | Loads GloVe `.txt` file, normalizes to unit vectors. |
| 2 | `get_embedding_for_word(word)` | ~15 | âšª NOT IMPORTANT | â€” | Lookup in vocab dict. Returns None if not found. |
| 3 | `semantic_distance(v1, v2)` | ~20 | âšª NOT IMPORTANT | â€” | `np.dot(v1, v2.T)` â€” cosine similarity (vectors are pre-normalized). |
| 4 | `get_lang_model_feature_size()` | ~25 | âšª NOT IMPORTANT | â€” | Returns embedding dimension (e.g., 100 for glove.6B.100d). |

---

## ontomatch/onto_parser.py
*428 lines â€” OWL ontology parser (ontospy)*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `OntoHandler` class | all | âšª NOT IMPORTANT | â€” | Wraps `ontospy` library. Parses OWL ontologies, caches via pickle. |
| 2 | `parse_ontology()` / `store_ontology()` / `load_ontology()` | ~30-70 | âšª NOT IMPORTANT | â€” | OWL file parsing and caching. |
| 3 | `classes()` / `class_hierarchy_iterator()` | ~80-110 | âšª NOT IMPORTANT | â€” | Iterate ontology class hierarchy. |
| 4 | `ancestors_of_class()` / `parents_of_class()` / `descendants_of_class()` | ~120-160 | âšª NOT IMPORTANT | â€” | Hierarchy traversal. |
| 5 | `compute_classes_signatures()` | ~230 | âšª NOT IMPORTANT | â€” | MinHash signatures of class name groups per hierarchy level. |
| 6 | `bow_repr_of(class_name)` | ~370 | âšª NOT IMPORTANT | â€” | Bag-of-words representation from class description + properties. |

---

## ontomatch/no_matcher.py
*~100 lines â€” Wikipedia text matching experiment*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `find_matching_to_text()` | all | âšª NOT IMPORTANT | â€” | Matches DB attributes to Wikipedia titles using GloVe semantic similarity. Pure experiment. |

---

## knowledgerepr/lite_graph.py
*~60 lines â€” Bitarray-based graph (alternative to NetworkX)*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `EdgeType` enum | ~10 | âšª NOT IMPORTANT | â€” | SCHEMA_SIM, CONTENT_SIM, PKFK, SEMANTIC. |
| 2 | `LiteGraph` class | ~20 | âšª NOT IMPORTANT | â€” | Adjacency list using `bitarray` for edge types. Never used in production (NetworkX is used instead). |
| 3 | `add_edge()` / `add_undirected_edge()` / `neighbors()` | ~30-55 | âšª NOT IMPORTANT | â€” | Graph operations on bitarray representation. Prototype code. |

---

## inputoutput/inputoutput.py
*~15 lines â€” Generic pickle serialization*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `serialize_object(obj, path)` | ~5 | ðŸŸ¡ MODERATE | â€” | `pickle.dump`. Used for LSH indexes and network serialization. |
| 2 | `deserialize_object(path)` | ~10 | ðŸŸ¡ MODERATE | â€” | `pickle.load`. Used to restore LSH indexes. |

---

## api/annotation.py
*~100 lines â€” Metadata annotation types*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `MDClass` enum | ~10 | âšª NOT IMPORTANT | â€” | WARNING, INSIGHT, QUESTION. |
| 2 | `MDRelation` enum | ~15 | âšª NOT IMPORTANT | â€” | MEANS_SAME_AS, MEANS_DIFF_FROM, IS_SUBCLASS_OF, etc. |
| 3 | `MDHit` namedtuple | ~25 | âšª NOT IMPORTANT | â€” | Metadata hit: id, author, md_class, text, source, target, relation. |
| 4 | `MDComment` namedtuple | ~50 | âšª NOT IMPORTANT | â€” | Comment: id, author, text, ref_id. |
| 5 | `MRS` class (Metadata Result Set) | ~70 | âšª NOT IMPORTANT | â€” | Iterator over metadata results. |

---

## api/reporting.py
*~80 lines â€” Network statistics*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `Report.__init__(network)` | ~10 | âšª NOT IMPORTANT | â€” | Computes stats on init. |
| 2 | `compute_all_statistics()` | ~30 | âšª NOT IMPORTANT | â€” | Counts tables, columns, edges by relation type. |
| 3 | `print_content_sim_relations()` / `print_schema_sim_relations()` / `print_pkfk_relations()` | ~50 | âšª NOT IMPORTANT | â€” | Debug printing. |
| 4 | `print_all_indexed_tables()` | ~60 | âšª NOT IMPORTANT | â€” | Lists all table names. |

---

## sugar.py
*~140 lines â€” REPL convenience shortcuts*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | Short variable aliases (`source`, `field`, `content`, `schema_sim`, etc.) | ~20-35 | âšª NOT IMPORTANT | â€” | Convenience bindings for interactive use. |
| 2 | `search(kws, contexts)` | ~50 | âšª NOT IMPORTANT | â€” | Deprecated. Multi-keyword, multi-context search wrapper. |
| 3 | `neighbors(i_drs, relations)` | ~75 | âšª NOT IMPORTANT | â€” | Deprecated. Multi-relation neighbor search. |
| 4 | `path(drs_a, drs_b, relation)` | ~110 | âšª NOT IMPORTANT | â€” | Deprecated. Path-finding wrapper. |
| 5 | `provenance(i_drs)` | ~130 | âšª NOT IMPORTANT | â€” | Deprecated. Provenance graph edge getter. |

---

## main.py
*~60 lines â€” System initialization and IPython shell*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `init_system(path, create_reporting)` | ~30 | ðŸŸ¡ MODERATE | `lakeagent/cli.py` | Deserializes network, creates StoreHandler, returns `(api, reporting)`. This is how you boot the system. |
| 2 | `__init_system()` (old API variant) | ~20 | âšª NOT IMPORTANT | â€” | Uses `ddapi.API` instead of `algebra.API`. Legacy. |
| 3 | `main()` | ~50 | âšª NOT IMPORTANT | â€” | Launches IPython embedded shell. |

---

## run_dod.py
*~20 lines â€” DoD CLI entry point*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `argparse` CLI | all | âšª NOT IMPORTANT | `lakeagent/cli.py` | Parses `--model_path`, `--separator`, `--output_path`, `--list_attributes`, `--list_values`. Calls `dod.main(args)`. |

---

## server_config.py
*~5 lines â€” Hardcoded server paths*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `path_model` / `separator` | all | âšª NOT IMPORTANT | `config.py` | Hardcoded paths. Machine-specific. |

---

## server-api/app.py
*~230 lines â€” Flask web API for DoD*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `/findvs` POST endpoint | ~60 | ðŸŸ¡ MODERATE | â€” | Parses grid payload â†’ `list_attributes` + `list_samples`, calls `dod.virtual_schema_iterative_search()`, returns first view as HTML table + analysis + join graph metadata. |
| 2 | `/next_view` POST endpoint | ~100 | ðŸŸ¡ MODERATE | â€” | Calls `next(view_generator)` to get next materialized view. |
| 3 | `/suggest_field` POST endpoint | ~130 | ðŸŸ¡ MODERATE | â€” | Calls `dod.aurum_api.suggest_schema(input_text)`. |
| 4 | `/download_view` POST endpoint | ~145 | âšª NOT IMPORTANT | â€” | Saves current view to CSV. Hardcoded path. |
| 5 | `obtain_view_analysis(view)` | ~160 | âšª NOT IMPORTANT | â€” | Per-column `df.describe().to_html()`. |
| 6 | `Ack` / `InvalidUsage` classes | ~180 | âšª NOT IMPORTANT | â€” | Flask error handling boilerplate. |

---

## aurum_cli.py
*283 lines â€” Fire-based CLI*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | `CSVDataSource` / `DBDataSource` dataclasses | ~30-100 | âšª NOT IMPORTANT | â€” | Data source config generation (YAML). |
| 2 | `AurumWrapper` class | ~120 | âšª NOT IMPORTANT | â€” | Filesystem-based data source and model management. |
| 3 | `AurumCLI.profile()` | ~210 | âšª NOT IMPORTANT | â€” | Invokes Java DDProfiler via `subprocess`. |
| 4 | `AurumCLI.build_model()` | ~220 | âšª NOT IMPORTANT | â€” | Invokes `networkbuildercoordinator.py` via subprocess. |
| 5 | `AurumCLI.export_model()` | ~230 | âšª NOT IMPORTANT | â€” | Exports to Neo4j via `Neo4jExporter`. |
| 6 | `AurumCLI.clear_store()` | ~250 | âšª NOT IMPORTANT | â€” | Deletes ES indices. |
| 7 | `AurumCLI.explore_model()` | ~260 | âšª NOT IMPORTANT | â€” | Opens IPython with loaded model. |

---

## export_network_2_neo4j.py
*~15 lines â€” Neo4j export entry point*

| # | Feature / Symbol | Lines | Importance | LakeAgent Equivalent | Notes |
|---|---|---|---|---|---|
| 1 | CLI + `serialize_network_to_neo4j()` | all | âšª NOT IMPORTANT | â€” | Thin wrapper to export FieldNetwork to Neo4j. Infrastructure only. |

---

## Quick-Reference: What Matters for Debugging LakeAgent

### ðŸ”´ Must-Know Algorithms (cross-reference these first)

| Aurum File | Feature | LakeAgent File | What to check |
|---|---|---|---|
| `networkbuilder.py` | `build_schema_sim_relation` | `graph/network_builder.py` | TF-IDF + LSH thresholds, edge creation |
| `networkbuilder.py` | `build_content_sim_mh_text` | `graph/network_builder.py` | MinHash threshold=0.7, num_perm=512 |
| `networkbuilder.py` | `build_content_sim_relation_num_overlap_distr` | `graph/network_builder.py` | MedianÂ±IQR overlap=0.85, inclusion=0.3, DBSCAN eps=0.1 |
| `networkbuilder.py` | `build_pkfk_relation` | `graph/network_builder.py` | Cardinality ratio > 0.7 |
| `dod.py` | `virtual_schema_iterative_search` | `discovery/data_on_demand.py` | 5-stage pipeline, greedy set cover |
| `dod.py` | `joinable()` | `discovery/data_on_demand.py` | Join graph enumeration via itertools.product |
| `data_processing_utils.py` | `materialize_join_graph` | `materialization/join_engine.py` | Tree-fold join strategy |
| `data_processing_utils.py` | `join_ab_on_key_optimizer` | `materialization/join_engine.py` | Memory-aware chunked join with 3-min timeout |
| `apiutils.py` | `DRS` class | `discovery/result_set.py` | Set operations, mode switching, iteration |
| `apiutils.py` | `Hit` + `compute_field_id` | `discovery/result_set.py` | ID hashing, equality semantics |
| `fieldnetwork.py` | `neighbors_id()` | `graph/field_network.py` | Core graph traversal |
| `fieldnetwork.py` | `find_path_hit()` / `find_path_table()` | `graph/field_network.py` | DFS path finding with provenance |
| `algebra.py` | `__neighbor_search()` | `discovery/algebra.py` | Inputâ†’DRS conversion + traversal |
| `config.py` | All thresholds | `lakeagent/config.py` | join_overlap_th=0.4, k=512 |

### âšª Safe to Ignore

- **Entire `ontomatch/` directory** â€” Ontology matching system. Not ported.
- **`sugar.py`** â€” REPL shortcuts, all deprecated.
- **`api/annotation.py`** â€” Metadata annotation system.
- **`api/reporting.py`** â€” Statistics printing.
- **`lite_graph.py`** â€” Unused bitarray graph prototype.
- **`export_network_2_neo4j.py`** â€” Neo4j export utility.
- **`aurum_cli.py`** â€” Fire CLI (calls subprocess for Java profiler).
- **`server-api/app.py`** â€” Flask endpoints (useful to understand the API contract, but not for algorithm debugging).
